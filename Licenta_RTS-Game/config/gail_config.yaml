behaviors:
  HallAgent:
    trainer_type: ppo
    hyperparameters:
      batch_size: 128
      buffer_size: 100
      learning_rate: 0.7
      beta: 5.0e-4
      epsilon: 0.2
      lambd: 0.99
      num_epoch: 10
      learning_rate_schedule: linear
    network_settings:
      normalize: false
      hidden_units: 256
      num_layers: 4
    reward_signals:
      extrinsic:
        gamma: 0.99
        strength: 1.0
    max_steps: 500000
    time_horizon: 64
    summary_freq: 10000
  DockAgent:
    trainer_type: ppo
    hyperparameters:
      batch_size: 128
      buffer_size: 100
      learning_rate: 0.7
      beta: 5.0e-4
      epsilon: 0.2
      lambd: 0.99
      num_epoch: 10
      learning_rate_schedule: linear
    network_settings:
      normalize: false
      hidden_units: 256
      num_layers: 4
    reward_signals:
      extrinsic:
        gamma: 0.99
        strength: 1.0
    max_steps: 500000
    time_horizon: 64
    summary_freq: 10000
  IdleBuildingAgent:
    trainer_type: ppo
    hyperparameters:
      batch_size: 128
      buffer_size: 100
      learning_rate: 0.7
      beta: 5.0e-4
      epsilon: 0.2
      lambd: 0.99
      num_epoch: 10
      learning_rate_schedule: linear
    network_settings:
      normalize: false
      hidden_units: 256
      num_layers: 4
    reward_signals:
      extrinsic:
        gamma: 0.99
        strength: 1.0
    max_steps: 500000
    time_horizon: 64
    summary_freq: 10000
  RefineryAgent:
    trainer_type: ppo
    hyperparameters:
      batch_size: 128
      buffer_size: 100
      learning_rate: 0.7
      beta: 5.0e-4
      epsilon: 0.2
      lambd: 0.99
      num_epoch: 10
      learning_rate_schedule: linear
    network_settings:
      normalize: false
      hidden_units: 256
      num_layers: 4
    reward_signals:
      extrinsic:
        gamma: 0.99
        strength: 1.0
    max_steps: 500000
    time_horizon: 64
    summary_freq: 10000
  UniversityAgent:
    trainer_type: ppo
    hyperparameters:
      batch_size: 128
      buffer_size: 100
      learning_rate: 0.7
      beta: 5.0e-4
      epsilon: 0.2
      lambd: 0.99
      num_epoch: 10
      learning_rate_schedule: linear
    network_settings:
      normalize: false
      hidden_units: 256
      num_layers: 4
    reward_signals:
      extrinsic:
        gamma: 0.99
        strength: 1.0
    max_steps: 500000
    time_horizon: 64
    summary_freq: 10000
  WarFactoryAgent:
    trainer_type: ppo
    hyperparameters:
      batch_size: 128
      buffer_size: 100
      learning_rate: 0.7
      beta: 5.0e-4
      epsilon: 0.2
      lambd: 0.99
      num_epoch: 10
      learning_rate_schedule: linear
    network_settings:
      normalize: false
      hidden_units: 256
      num_layers: 4
    reward_signals:
      extrinsic:
        gamma: 0.99
        strength: 1.0
    max_steps: 500000
    time_horizon: 64
    summary_freq: 10000
  AttackingAgent:
    trainer_type: ppo
    hyperparameters:
      batch_size: 128
      buffer_size: 100
      learning_rate: 0.7
      beta: 5.0e-4
      epsilon: 0.2
      lambd: 0.99
      num_epoch: 10
      learning_rate_schedule: linear
    network_settings:
      normalize: false
      hidden_units: 256
      num_layers: 4
    reward_signals:
      extrinsic:
        gamma: 0.99
        strength: 1.0
    max_steps: 500000
    time_horizon: 64
    summary_freq: 10000
  CargoAgent:
    trainer_type: ppo
    hyperparameters:
      batch_size: 128
      buffer_size: 100
      learning_rate: 0.7
      beta: 5.0e-4
      epsilon: 0.2
      lambd: 0.99
      num_epoch: 10
      learning_rate_schedule: linear
    network_settings:
      normalize: false
      hidden_units: 256
      num_layers: 4
    reward_signals:
      extrinsic:
        gamma: 0.99
        strength: 1.0
    max_steps: 500000
    time_horizon: 64
    summary_freq: 10000
  IdleUnitAgent:
    trainer_type: ppo
    hyperparameters:
      batch_size: 128
      buffer_size: 100
      learning_rate: 0.7
      beta: 5.0e-4
      epsilon: 0.2
      lambd: 0.99
      num_epoch: 10
      learning_rate_schedule: linear
    network_settings:
      normalize: false
      hidden_units: 256
      num_layers: 4
    reward_signals:
      extrinsic:
        gamma: 0.99
        strength: 1.0
    max_steps: 500000
    time_horizon: 64
    summary_freq: 10000
  HarvesterAgent:
    trainer_type: ppo
    hyperparameters:
      batch_size: 128
      buffer_size: 100
      learning_rate: 0.7
      beta: 5.0e-4
      epsilon: 0.2
      lambd: 0.99
      num_epoch: 10
      learning_rate_schedule: linear
    network_settings:
      normalize: false
      hidden_units: 256
      num_layers: 4
    reward_signals:
      extrinsic:
        gamma: 0.99
        strength: 1.0
    max_steps: 500000
    time_horizon: 64
    summary_freq: 10000
  WorkerAgent:
    trainer_type: ppo
    hyperparameters:
      batch_size: 128
      buffer_size: 100
      learning_rate: 0.7
      beta: 5.0e-4
      epsilon: 0.2
      lambd: 0.99
      num_epoch: 10
      learning_rate_schedule: linear
    network_settings:
      normalize: false
      hidden_units: 256
      num_layers: 4
    reward_signals:
      gail:
        gamma: 0.99
        strength: 1.0
        network_settings:
          normalize: false
          hidden_units: 256
          num_layers: 4
          vis_encode_type: simple
        learning_rate: 0.7
        use_actions: false
        use_vail: false
        demo_path: demos/WorkerDemoO.demo
      extrinsic:
        gamma: 0.99
        strength: 1.0
    max_steps: 500000
    time_horizon: 64
    summary_freq: 10000